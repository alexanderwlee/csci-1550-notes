\documentclass{amsart}

\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}

\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{corollary}{Corollary}
\newtheorem*{axiom}{Axiom}

\DeclarePairedDelimiter\abs{\lvert}{\rvert} % absolute value
% Swap the definition of \abs*, so that \abs
% resizes the size of the bars, and the starred version does not.
\makeatletter
\let\oldabs\abs%
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\makeatother

\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\Var}{\mathbf{Var}}
\newcommand{\Cov}{\mathbf{Cov}}
\newcommand{\st}{\mathrel{:}}

\title{CSCI 1550: Probabilistic Methods in CS \\ \small{Notes}}
\author{Alexander W. Lee}

\begin{document}

\maketitle

% =================== Chapter 1 =================== 

\section{Events and Probability}

\begin{definition}
  A probability space has three components:
  \begin{enumerate}
    \item a sample space $\Omega$, which is the set of all possible outcomes of
      the random process modeled by the probability space;
    \item a family of sets $\mathcal{F}$ representing the allowable events,
      where each set in $\mathcal{F}$ is a subset of the sample space $\Omega$;
      and
    \item a probability function $\Pr: \mathcal{F} \to \R$ satisfying the
      following definition.
  \end{enumerate}
\end{definition}

\begin{definition}
  A probability function is any function $\Pr: \mathcal{F} \to \R$ that
  satisfies the following conditions:
  \begin{enumerate}
    \item for any event $E$, $0 \leq \Pr(E) \leq 1$;
    \item $\Pr(\Omega) = 1$; and
    \item for any finite or countably infinite sequence of pairwise mutually
      disjoint events $E_1, E_2, E_3, \ldots$,
      \[
        \Pr \left( \bigcup_{i \geq 1} E_i \right) = \sum_{i \geq 1} \Pr(E_i).
      \]
  \end{enumerate}
\end{definition}

\begin{lemma}
  For any two events $E_1$ and $E_2$,
  \[
    \Pr(E_1 \cup E_2) = \Pr(E_1) + \Pr(E_2) - \Pr(E_1 \cap E_2).
  \]
\end{lemma}

\begin{lemma}[Union Bound]
  For any finite or countably infinite sequence of events $E_1, E_2, \ldots$,
  \[
    \Pr \left( \bigcup_{i \geq 1} E_i \right) \leq \sum_{i \geq 1} \Pr(E_i).
  \]
\end{lemma}

\begin{lemma}
  Let $E_1, \ldots, E_n$ be any $n$ events. Then
  \begin{align*}
    \Pr \left( \bigcup_{i = 1}^n E_i \right) = & \sum_{i = 1}^n \Pr(E_i) - \sum_{i
    < j} \Pr(E_i \cap E_j) + \sum_{i < j < k} \Pr(E_i \cap E_j \cap E_k) \\
    &- \cdots + {(-1)}^{l + 1} \sum_{i_1 < i_2 < \cdots < i_l} \Pr \left(
    \bigcap_{r = 1}^l E_{i_r} \right) + \cdots.
  \end{align*}
\end{lemma}

\begin{definition}
  Two events $E$ and $F$ are independent if and only if
  \[
    \Pr(E \cap F) = \Pr(E) \cdot \Pr(F).
  \]
  More generally, events $E_1, E_2, \ldots, E_k$ are mutually independent if and
  only if, for any subset $I \subseteq [1, k]$,
  \[
    \Pr \left( \bigcap_{i \in I} E_i \right) = \prod_{i \in I} \Pr(E_i).
  \]
\end{definition}

\begin{definition}
  The conditional probability that event $E$ occurs given that event $F$ occurs
  is
  \[
    \Pr(E \mid F) = \frac{\Pr(E \cap F)}{\Pr(F)}.
  \]
  The conditional probability is well-defined only if $\Pr(F) > 0$.
\end{definition}

\begin{theorem}[Law of Total Probability]
  Let $E_1, E_2, \ldots, E_n$ be mutually disjoint events in the sample space
  $\Omega$, and let $\bigcup_{i = 1}^n E_i = \Omega$. Then
  \[
    \Pr(B) = \sum_{i = 1}^n \Pr(B \cap E_i) = \sum_{i = 1}^n \Pr(B \mid E_i)
    \Pr(E_i).
  \]
\end{theorem}

\begin{theorem}[Bayes' Law]
  Assume that $E_1, E_2, \ldots, E_n$ are mutually disjoint events in the sample
  space $\Omega$ such that $\bigcup_{i = 1}^n E_i = \Omega$. Then
  \[
    \Pr(E_j \mid B) = \frac{\Pr(E_j \cap B)}{\Pr(B)} = \frac{\Pr(B \mid E_j)
    \Pr(E_j)}{\sum_{i = 1}^n \Pr(B \mid E_i) \Pr(E_i)}.
  \]
\end{theorem}

\begin{theorem}
  For any $x \in \R$,
  \[
    1 - x \leq e^{-x}.
  \]
  Equivalently,
  \[
    1 + x \leq e^x.
  \]
\end{theorem}

% =================== Chapter 2 =================== 

\section{Discrete Random Variables and Expectation}

\begin{definition}
  A random variable $X$ on a sample space $\Omega$ is a real-valued (measurable)
  function on $\Omega$; that is, $X : \Omega \to \R$. A discrete random variable
  is a random variable that takes on only a finite or countably infinite numbers
  of values.
\end{definition}

\begin{definition}
  Two random variable $X$ and $Y$ are independent if and only if
  \[
    \Pr((X = x) \cap (Y = y)) = \Pr(X = x) \cdot \Pr(Y = y)
  \]
  for all values $x$ and $y$. Similarly, random variables $X_1, X_2, \ldots,
  X_k$ are mutually independent if and only if, for any subset $I \subseteq [1,
  k]$ and any values $x_i, i \in I$,
  \[
    \Pr \left( \bigcap_{i \in I} (X_i = x_i) \right) = \prod_{i \in I} \Pr(X_i =
    x_i).
  \]
\end{definition}

\begin{definition}
  The expectation of a discrete random variable $X$, denoted by $\E[X]$, is
  given by
  \[
    \E[X] = \sum_i i \Pr(X = i),
  \]
  where the summation is over all values in the range of $X$. The expectation is
  finite if $\sum_i \abs{i} \Pr(X = i)$ converges; otherwise, the expectation is
  unbounded.
\end{definition}

\begin{theorem}[Linearity of Expectations]
  For any finite collection of discrete random variables $X_1, X_2, \ldots, X_n$
  with finite expectations,
  \[
    \E \left[ \sum_{i = 1}^n X_i \right] = \sum_{i = 1}^n \E[X_i].
  \]
\end{theorem}

\begin{lemma}
  For any constant $c$ and discrete random variable $X$,
  \[
    \E[cX] = c\E[X].
  \]
\end{lemma}

\begin{definition}
  Suppose that we run an experiment that succeeds with probability $p$ and fails
  with probability $1 - p$. A variable $X$ is called a \emph{Bernoulli} or an
  \emph{indicator} random variable if
  \[
    X =
    \begin{cases}
      1 & \text{if the experiments succeeds,} \\
      0 & \text{otherwise.}
    \end{cases}
  \]
  Note that $\E[X] = \Pr(X = 1) = p$.
\end{definition}

\begin{definition}
  Consider a sequence of $n$ independent experiments, each of which succeeds
  with probability $p$. If we let $X$ represent the number of successes in the
  $n$ experiments, then $X$ has a binomial distribution. A binomial random
  variable $X$ with parameters $n$ and $p$, denoted by $B(n, p)$, is defined by
  the following probability distribution on $j = 0, 1, 2, \ldots, n$:
  \[
    \Pr(X = j) = \binom{n}{j} p^j {(1 - p)}^{n - j}.
  \]
  That is, the binomial random variable $X$ equals $j$ when there are exactly
  $j$ successes and $n - j$ failures in $n$ independent experiments, each of
  which is successful with probability $p$.
\end{definition}

\begin{lemma}
  For a binomial random variable $X$ with parameters $n$ and $p$,
  \[
    \E[X] = np.
  \]
\end{lemma}

\begin{definition}
  \[
    \E[Y \mid Z = z] = \sum_y y \Pr(Y = y \mid Z = z),
  \]
  where the summation is over all $y$ in the range of $Y$.
\end{definition}

\begin{lemma}
  For any random variables $X$ and $Y$,
  \[
    \E[X] = \sum_y \Pr(Y = y) \E[X \mid Y = y],
  \]
  where the sum is over all values in the range of $Y$ and all of the
  expectations exist.
\end{lemma}

\begin{lemma}
  For any finite collection of discrete random variables $X_1, X_2, \ldots, X_n$
  with finite expectations and for any random variable $Y$,
  \[
    \E \left[ \sum_{i = 1}^n X_i \mid Y = y \right] = \sum_{i = 1}^n \E[X_i \mid
    Y = y].
  \]
\end{lemma}

\begin{definition}
  The expression $\E[X \mid Y]$ is a random variable $f(Z)$ that takes on the
  value $\E[Y \mid Z = z]$ when $Z = z$.
\end{definition}

\begin{theorem}
  \[
    \E[Y] = \E[\E[Y \mid Z]].
  \]
\end{theorem}

\begin{definition}
  We perform a sequence of independent trials until the first success, where
  each trial succeeds with probability $p$. A geometric random variable $X$ with
  parameter $p$ is given by the following probability distribution on $n = 1, 2,
  \ldots$,:
  \[
    \Pr(X = n) = {(1 - p)}^{n - 1} p.
  \]
  That is, for the geometric random variable $X$ to equal $n$, there must be $n
  - 1$ failures, followed by a success.
\end{definition}

\begin{lemma}
  For a geometric random variable $X$ with parameter $p$ and for $n > 0$,
  \[
    \Pr(X = n + k \mid X > k) = \Pr(X = n).
  \]
\end{lemma}

\begin{lemma}
  Let $X$ be a discrete random variable that takes on only non-negative integer
  values. Then
  \[
    \E[X] = \sum_{i = 1}^{\infty} \Pr(X \geq i).
  \]
\end{lemma}

\begin{lemma}
  For a geometric random variable $X$ with parameter $p$,
  \[
    \E[X] = \frac{1}{p}.
  \]
\end{lemma}

\begin{lemma}
  The harmonic number $H(n) = \sum_{i = 1}^n 1/i$ satisfies $H(n) = \ln n +
  \Theta(1)$.
\end{lemma}


% =================== Chapter 3 =================== 

\section{Moments and Deviations}

\begin{theorem}[Markov's Inequality]
  Let $X$ be a random variable that assumes only non-negative values. Then, for
  all $a > 0$,
  \[
    \Pr(X \geq a) \leq \frac{\E[X]}{a}.
  \]
\end{theorem}

\begin{definition}
  The kth moment of a random variable $X$ is $\E[X^k]$.
\end{definition}

\begin{definition}
  The \emph{variance} of a random variable $X$ is defined as
  \[
    \Var[X] = \E[{(X - \E[X])}^2] = \E[X^2] - {(\E[X])}^2.
  \]
  The \emph{standard deviation} of a random variable $X$ is
  \[
    \sigma[X] = \sqrt{\Var[X]}.
  \]
\end{definition}

\begin{lemma}
  For a Bernoulli random variable with success probability $p$,
  \[
    \Var[X] = p(1 - p).
  \]
\end{lemma}

\begin{definition}
  The \emph{covariance} of two random variables $X$ and $Y$ is
  \[
    \Cov(X, y) = \E[(X - \E[X]) (Y - \E[Y])].
  \]
\end{definition}

\begin{theorem}
  For any two random variables $X$ and $Y$,
  \[
    \Var[X + Y] = \Var[X] + \Var[Y] + 2 \Cov(X, Y).
  \]
\end{theorem}

\begin{theorem}
  If $X$ and $Y$ are two independent random variables, then
  \[
    \E[X \cdot Y] = \E[X] \cdot \E[Y].
  \]
\end{theorem}

\begin{corollary}
  If $X$ and $Y$ are independent random variables, then
  \[
    \Cov(X, Y) = 0
  \]
  and
  \[
    \Var[X + Y] = \Var[X] + \Var[Y].
  \]
\end{corollary}

\begin{theorem}
  Let $X_1, X_2, \ldots, X_n$ be mutually independent random variables. Then
  \[
    \Var \left[ \sum_{i = 1}^n X_i \right] = \sum_{i = 1}^n \Var[X_i].
  \]
\end{theorem}

\begin{lemma}
  For a binomial random variable $X$ with parameters $n$ and $p$,
  \[
    \Var[X] = np(1 - p).
  \]
\end{lemma}

\begin{theorem}[Chebyshev's Inequality]
  For any $a > 0$,
  \[
    \Pr(\abs{X - \E[X]} \geq a) \leq \frac{\Var[X]}{a^2}.
  \]
\end{theorem}

\begin{corollary}
  For any $t > 1$,
  \begin{align*}
    \Pr(\abs{X - \E[X]} &\geq t \cdot \sigma[X]) \leq \frac{1}{t^2}\ \text{and} \\
    \Pr(\abs{X - \E[X]} &\geq t \cdot \E[X]) \leq \frac{\Var[X]}{t^2
    {(\E[X])}^2}.
  \end{align*}
\end{corollary}

\begin{lemma}
  For a geometric random variable $X$ with parameter $p$,
  \[
    \Var[X] = (1 - p) / p^2.
  \]
\end{lemma}

\begin{definition}
  The $X$ be a random variable. The \emph{median} of $X$ is defined to be any
  value $m$ such that
  \[
    \Pr(X \leq m) \geq \frac{1}{2}\quad \text{and}\quad \Pr(X \geq m) \geq
    \frac{1}{2}.
  \]
\end{definition}

% =================== Chapter 4 =================== 

\section{Chernoff and Hoeffding Bounds}

\begin{definition}
  The \emph{moment generating function} of a random variable $X$ is
  \[
    M_X(t) = \E[e^{tX}].
  \]
\end{definition}

\begin{theorem}
  Let $X$ be a random variable with moment generating function $M_X(t)$. Under
  the assumption that exchanging the expectation and differentiation operands is
  legitimate, for all $n > 1$ we have
  \[
    \E[X^n] = M_X^{(n)}(0),
  \]
  where $M_X^{(n)}(0)$ is the \emph{$n$th derivative} of $M_X(t)$ evaluated at
  $t = 0$.
\end{theorem}

\begin{theorem}
  Let $X$ and $Y$ be two random variables. If
  \[
    M_X(t) = M_Y(t)
  \]
  for all $t \in (-\delta, \delta)$ for some $\delta > 0$, then $X$ and $Y$ have
  the same distribution.
\end{theorem}

\begin{theorem}
  If $X$ and $Y$ are independent random variables, then
  \[
    M_{X + Y}(t) = M_X(t) M_Y(t).
  \]
\end{theorem}

\begin{definition}
  A sum of independent 0-1 random variables are known as \emph{Poisson trials}. %chktex 8
  The distributions of the random variables in Poisson trials are not
  necessarily identical.\ \emph{Bernoulli trials} are a special case of Poisson
  trials where the independent 0-1 random variables have the same distribution; %chktex 8
  in other words, all trials are Poisson trials that take on the value 1 with
  the same probability.
\end{definition}

\begin{theorem}
  Let $X_1, \ldots, X_n$ be independent Poisson trials such that $\Pr(X_i = 1) =
  p_i$. Let $X = \sum_{i=1}^n X_i$ and $\mu = \E[X]$. Then the following
  Chernoff bounds hold:
  \begin{enumerate}
    \item for any $\delta > 0$,
      \[
        \Pr(X \geq (1 + \delta) \mu) \leq {\left( \frac{e^\delta}{{(1 +
        \delta)}^{(1 + \delta)}} \right)}^\mu;
      \]
    \item for $0 < \delta \leq 1$,
      \[
        \Pr(X \geq (1 + \delta) \mu) \leq e^{-\mu \delta^2 / 3};
      \]
    \item for $R \geq 6 \mu$,
      \[
        \Pr(X \geq R) \leq e^{-R}.
      \]
  \end{enumerate}
\end{theorem}

\begin{theorem}
  Let $X_1, \ldots, X_n$ be independent Poisson trials such that $\Pr(X_i = 1) =
  p_i$. Let $X = \sum_{i=1}^n X_i$ and $\mu = \E[X]$. Then, for $0 < \delta <
  1$:
  \begin{enumerate}
    \item 
      \[
        \Pr(X \leq (1 - \delta) \mu) \leq {\left( \frac{e^{-\delta}}{{(1 -
        \delta)}^{(1 - \delta)}} \right)}^\mu;
      \]
    \item
      \[
        \Pr(X \leq (1 - \delta) \mu) \leq e^{-\mu \delta^2 / 2}
      \]
  \end{enumerate}
\end{theorem}

\begin{corollary}
  Let $X_1, \ldots, X_n$ be independent Poisson trials such that $\Pr(X_i = 1) =
  p_i$. Let $X = \sum_{i=1}^n X_i$ and $\mu = \E[X]$. For $0 < \delta < 1$,
  \[
    \Pr(\abs{X - \mu} \geq \delta \mu) \leq 2 e^{-\mu \delta^2 / 3}.
  \]
\end{corollary}

\begin{theorem}
  Let $X_1, \ldots, X_n$ be independent random variables with
  \[
    \Pr(X_i = 1) = \Pr(X_i = -1) = \frac{1}{2}.
  \]
  Let $X = \sum_{i=1}^n X_i$. For any $a > 0$,
  \[
    \Pr(X \geq a) \leq e^{-a^2 / 2n}.
  \]
  By symmetry we also have
  \[
    \Pr(X \leq -a) \leq e^{-a^2 / 2n}.
  \]
\end{theorem}

\begin{corollary}
  Let $X_1, \ldots X_n$ be independent random variables with
  \[
    \Pr(X_i = 1) = \Pr(X_i = -1) = \frac{1}{2}.
  \]
  Let $X = \sum_{i=1}^n X_i$. Then, for any $a > 0$,
  \[
    \Pr(|X| \geq a) \leq 2e^{-a^2 / 2n}.
  \]
\end{corollary}

\begin{corollary}
  Let $Y_1, \ldots, Y_n$ be independent random variables with
  \[
    \Pr(Y_i = 1) = \Pr(Y_i = 0) = \frac{1}{2}.
  \]
  Let $Y = \sum_{i=1}^n Y_i$ and $\mu = \E[Y] = n / 2$.
  \begin{enumerate}
    \item For any $a > 0$,
      \[
        \Pr(Y \geq \mu + a) \leq e^{-2a^2 / n}.
      \]
    \item For any $\delta > 0$,
      \[
        \Pr(Y \geq (1 + \delta) \mu) \leq e^{-\delta^2 \mu}.
      \]
  \end{enumerate}
\end{corollary}

\begin{corollary}
  Let $Y_1, \ldots, Y_n$ be independent random variables with
  \[
    \Pr(Y_i = 1) = \Pr(Y_i = 0) = \frac{1}{2}.
  \]
  Let $Y = \sum_{i=1}^n Y_i$ and $\mu = \E[Y] = n / 2$.
  \begin{enumerate}
    \item For any $0 < a < \mu$,
      \[
        \Pr(Y \leq \mu - a) \leq e^{-2a^2 / n}.
      \]
    \item For any $0 < \delta < 1$,
      \[
        \Pr(Y \leq (1 - \delta) \mu) \leq e^{-\delta^2 \mu}.
      \]
  \end{enumerate}
\end{corollary}

\begin{theorem}[Hoeffding Bound]
  Let $X_1, \ldots, X_n$ be independent random variables such that for all $1
  \leq i \leq n$, $\E[X_i] = \mu$ and $\Pr(a \leq X_i \leq b) = 1$. Then
  \[
    \Pr \left( \abs{\frac{1}{n} \sum_{i=1}^n X_i - \mu} \geq \epsilon \right)
    \leq 2e^{-2n \epsilon^2 / {(b - a)}^2}.
  \]
\end{theorem}

\begin{lemma}[Hoeffding's Lemma]
  Let $X$ be a random variable such that $\Pr(X \in [a, b]) = 1$ and $\E[X] =
  0$. Then for every $\lambda > 0$,
  \[
    \E[e^{\lambda X}] \leq e^{\lambda^2 {(b - a)}^2 / 8}.
  \]
\end{lemma}

\begin{theorem}
  Let $X_1, \ldots, X_n$ be independent random variables with $\E[X_i] = \mu_i$
  and $\Pr(a_i \leq X_i \leq b_i) = 1$ for constants $a_i$ and $b_i$. Then
  \[
    \Pr \left( \abs{\sum_{i=1}^n X_i - \sum_{i=1}^n \mu_i} \geq \epsilon \right)
    \leq 2e^{-2 \epsilon^2 / \sum_{i=1}^n {(b_i - a_i)}^2}.
  \]
\end{theorem}

\end{document}
